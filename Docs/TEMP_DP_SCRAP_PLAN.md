# Plan: Drikpanchang.com Scraper for Full 1900-2050 Validation

## Context

We have ~720 dates manually verified against drikpanchang.com out of 55,152 days (1900-2050). The 55k-day reference CSVs are generated by our own C code — they're regression baselines, not independent ground truth. We want to scrape drikpanchang.com to obtain independent validation data for every day in the range.

**Goal**: Build a two-phase scraper (fetch + parse) that saves raw HTML and produces a CSV comparable to `validation/moshier/ref_1900_2050.csv`.

## Key Findings from Page Analysis

- **Date range**: drikpanchang supports dates from 3227 BCE to 3000 CE — our range is fine
- **Month page URL**: `https://www.drikpanchang.com/panchang/month-panchang.html?date=01/MM/YYYY`
  - Shows full Gregorian month grid with tithi + S/K per day
  - Header: Saka year, Purnimanta month name(s)
  - 1 fetch = ~30 days of tithi data
  - **1,812 fetches** for 1900-01 through 2050-12
- **Day page URL**: `https://www.drikpanchang.com/panchang/day-panchang.html?date=DD/MM/YYYY`
  - Shows full Hindu date: Chandramasa (Amanta/Purnimanta), Paksha, tithi, Saka year
  - 1 fetch = 1 day
- **Amanta/Purnimanta**: Default is Purnimanta. Toggle is JavaScript-based (`dpSettingsToolbar.handlePanchangSchoolOptionClick('amanta', true)`), likely sets a cookie. Need to discover the cookie.
- **Tithi data is scheme-independent** — same tithi name + S/K regardless of Amanta/Purnimanta setting

## Approach: Two-Phase (Fetch then Parse)

Separating fetch from parse is critical — raw HTML is saved permanently so we never re-fetch. The parser can be iterated and improved offline.

### Phase 1: Fetch Raw HTML

**Month pages** (primary — 1,812 fetches):
- Fetch every Gregorian month 1900-01 through 2050-12
- Save raw HTML to `scraper/data/raw/month/YYYY-MM.html`
- Skip already-downloaded files (resume capability)
- Configurable delay between requests (default 20s)
- At 20s average: ~10 hours

**Day pages** (supplementary — for masa/saka verification):
- Fetch day pages for every 1st of each Amanta month (S-1 Pratipada days)
- Also fetch adhika month boundary dates
- Estimated ~400-500 fetches
- Save to `scraper/data/raw/day/YYYY-MM-DD.html`
- Run after month pages complete

### Phase 2: Parse HTML (Offline)

**Month page parser**:
- Extract tithi name + Shukla/Krishna for each day cell
- Extract Saka year from header
- Extract Purnimanta month name(s) from header
- Convert tithi name → number (1-30): S-Pratipada=1 ... Purnima=15, K-Pratipada=16 ... Amavasya=30

**Masa derivation** (from month page data):
- Purnimanta→Amanta conversion: Purnimanta month M + Krishna paksha = Amanta month M-1
- Amavasya marks Amanta month boundary
- Adhika months: look for "Adhika" in header text

**Day page parser** (verification):
- Extract Chandramasa, Paksha, tithi, Saka year
- Cross-validate against month-page-derived values

**Output**: `scraper/data/parsed/drikpanchang_lunisolar.csv` with format: `year,month,day,tithi,masa,adhika,saka`

### Phase 3: Compare

- Diff drikpanchang CSV against `validation/moshier/ref_1900_2050.csv`
- Report mismatches with details
- Output: `scraper/data/comparison_report.txt`

## Directory Structure

```
scraper/
  README.md                              # Usage instructions
  requirements.txt                       # requests, beautifulsoup4
  fetch.py                               # Phase 1: download raw HTML
  parse.py                               # Phase 2: parse HTML → CSV
  compare.py                             # Phase 3: diff against our ref CSV
  config.py                              # Shared constants, mappings, paths
  data/
    raw/
      month/                             # YYYY-MM.html (1,812 files)
      day/                               # YYYY-MM-DD.html (optional, ~500 files)
    parsed/
      drikpanchang_lunisolar.csv         # Final parsed output
    comparison_report.txt                # Diff results
```

## Implementation Details

### config.py
- `TITHI_NAMES` dict: maps drikpanchang tithi names → numbers 1-30
  - {"Pratipada": 1, "Dwitiya": 2, ..., "Chaturdashi": 14, "Purnima": 15, "Amavasya": 30}
  - Shukla: use number as-is (1-15). Krishna: add 15 (16-29). Amavasya: 30.
- `MASA_NAMES` dict: Purnimanta month name → Amanta month number
- `BASE_URL_MONTH`, `BASE_URL_DAY`
- `DEFAULT_DELAY = 20` seconds
- `DATA_DIR`, `RAW_DIR`, `PARSED_DIR` paths

### fetch.py
- `fetch_month_pages(start_year, end_year, delay)` — main loop
- `requests.get()` with realistic User-Agent header
- Skip existing files (resume on restart)
- Progress logging: current month, total remaining, ETA
- Graceful shutdown on Ctrl+C (saves progress)
- `--start-year`, `--end-year`, `--delay` CLI args
- Priority ordering option: fetch years near present first (more likely to match drikpanchang's primary data)

### parse.py
- Bootstrap: first run on 1 saved HTML to discover DOM structure (class names, selectors)
- `parse_month_html(html_path) → list of (day, tithi_num, paksha)`
- `build_csv(year_range) → CSV file`
- Masa derivation logic from Purnimanta header + tithi sequence
- `--output` CLI arg for CSV path

### compare.py
- Load drikpanchang CSV and our ref CSV
- Compare field by field
- Summary: total days, matches, mismatches by field (tithi, masa, adhika, saka)
- Detail: first N mismatches with full context

## Step-by-Step Implementation Order

1. Create `scraper/` directory with `config.py` (constants, mappings)
2. Create `fetch.py` — fetch & save month HTML with delays + resume
3. **User runs fetch.py** — fetches first ~5 months manually to test
4. Create `parse.py` — examine saved HTML, build parser, test on those 5 months
5. Iterate parser until it correctly extracts tithi + masa for test months
6. **User runs fetch.py** for remaining months (background, ~10 hours)
7. Run `parse.py` on all saved HTML → full CSV
8. Create `compare.py` — diff against ref CSV
9. Analyze mismatches

## Verification

1. Fetch 5 sample months (e.g., 2025-01 through 2025-05) with short delay
2. Parse them and compare against our existing 186 hardcoded test dates
3. Confirm 100% match on known-good dates before running full fetch
4. After full parse, compare against ref CSV — expect 100% match (or discover real bugs)

## Notes

- Raw HTML files will be ~50-100 KB each → ~180 MB total for 1,812 months (manageable)
- Parser will need to handle the CSS grid layout (divs, not tables) — exact class names TBD from first saved HTML
- If drikpanchang rate-limits: increase delay, add exponential backoff
- The Amanta/Purnimanta cookie question: we'll try fetching without it first (tithi is scheme-independent), then discover the cookie by examining response headers if needed for masa
